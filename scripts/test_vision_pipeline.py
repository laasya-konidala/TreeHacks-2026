"""
Test the full agent pipeline: vision input ‚Üí confusion detection ‚Üí agent routing.

Run this while `python run.py` is running in another terminal.
Tests with and without GEMINI_API_KEY.

Usage:
  # Terminal 1: start the system
  python run.py

  # Terminal 2: run this test
  python scripts/test_vision_pipeline.py
"""
import os
import sys
import json
import time
import base64
import asyncio
import threading

import httpx

sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

BACKEND = "http://localhost:8080"
WS_URL = "ws://localhost:8080/ws"

# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ

def make_fake_screenshot() -> str:
    """Create a tiny 1x1 PNG as base64 for testing (avoids needing a real screenshot)."""
    # Minimal valid PNG (1x1 white pixel)
    import struct, zlib
    def create_minimal_png():
        sig = b'\x89PNG\r\n\x1a\n'
        ihdr_data = struct.pack('>IIBBBBB', 1, 1, 8, 2, 0, 0, 0)
        ihdr_crc = zlib.crc32(b'IHDR' + ihdr_data)
        ihdr = struct.pack('>I', 13) + b'IHDR' + ihdr_data + struct.pack('>I', ihdr_crc)
        raw = zlib.compress(b'\x00\xff\xff\xff')
        idat_crc = zlib.crc32(b'IDAT' + raw)
        idat = struct.pack('>I', len(raw)) + b'IDAT' + raw + struct.pack('>I', idat_crc)
        iend_crc = zlib.crc32(b'IEND')
        iend = struct.pack('>I', 0) + b'IEND' + struct.pack('>I', iend_crc)
        return sig + ihdr + idat + iend
    return base64.b64encode(create_minimal_png()).decode()


async def listen_ws(results: list, timeout: float = 15.0):
    """Listen on WebSocket for agent responses."""
    import websockets
    try:
        async with websockets.connect(WS_URL) as ws:
            start = time.time()
            while time.time() - start < timeout:
                try:
                    msg = await asyncio.wait_for(ws.recv(), timeout=2.0)
                    data = json.loads(msg)
                    results.append(data)
                    print(f"  üì® WS received: agent_type={data.get('agent_type')}, "
                          f"type={data.get('content_type')}, "
                          f"content={str(data.get('content', ''))[:80]}...")
                except asyncio.TimeoutError:
                    continue
    except Exception as e:
        print(f"  WS listener error: {e}")
        print("  (This is fine if websockets package is not installed ‚Äî using HTTP polling instead)")


# ‚îÄ‚îÄ‚îÄ Test Scenarios ‚îÄ‚îÄ‚îÄ

async def test_health():
    """Test 1: Backend is alive."""
    print("\n" + "=" * 50)
    print("TEST 1: Health Check")
    print("=" * 50)
    async with httpx.AsyncClient() as client:
        resp = await client.get(f"{BACKEND}/health", timeout=5.0)
        data = resp.json()
        print(f"  Status: {resp.status_code}")
        print(f"  Body: {data}")
        assert resp.status_code == 200
        print("  ‚úì PASS")


async def test_confusion_detection_local():
    """Test 2: Confusion detector works locally (no API needed)."""
    print("\n" + "=" * 50)
    print("TEST 2: Confusion Detection (local, no API)")
    print("=" * 50)

    from agents.confusion_detector import ConfusionDetector
    from agents.models import WorkContext

    detector = ConfusionDetector()

    # Scenario A: High confusion ‚Äî lots of deletion + long pause
    ctx_a = WorkContext(
        screen_content="def gradient_descent(learning_rate):\n    # ??? \n    pass",
        screen_content_type="code",
        detected_topic="gradient_descent",
        typing_speed_ratio=0.3,    # very slow
        deletion_rate=0.7,         # lots of backspacing
        pause_duration=15.0,       # long pause
        scroll_back_count=5,       # re-reading
        user_touched_agent=False,
        user_message="",
        user_id="test",
    )
    result_a = detector.score(ctx_a)
    print(f"  Scenario A (high confusion):")
    print(f"    Score:          {result_a.confusion_score:.2f}")
    print(f"    Should intervene: {result_a.should_intervene}")
    print(f"    Confusion type: {result_a.confusion_type}")
    print(f"    Signals:        {result_a.signals}")
    assert result_a.should_intervene, "Expected intervention for high confusion"
    print("  ‚úì PASS ‚Äî high confusion detected")

    # Scenario B: No confusion ‚Äî normal work
    ctx_b = WorkContext(
        screen_content="x = np.dot(weights, inputs) + bias",
        screen_content_type="code",
        detected_topic="neural_network",
        typing_speed_ratio=1.0,
        deletion_rate=0.05,
        pause_duration=2.0,
        scroll_back_count=0,
        user_touched_agent=False,
        user_message="",
        user_id="test",
    )
    result_b = detector.score(ctx_b)
    print(f"  Scenario B (normal work):")
    print(f"    Score:          {result_b.confusion_score:.2f}")
    print(f"    Should intervene: {result_b.should_intervene}")
    assert not result_b.should_intervene, "Should NOT intervene when user is working normally"
    print("  ‚úì PASS ‚Äî no false positive")

    # Scenario C: Explicit touch with visual keywords
    ctx_c = WorkContext(
        screen_content="‚à´‚ÇÄ^œÄ sin(x) dx = [-cos(x)]‚ÇÄ^œÄ",
        screen_content_type="equation",
        detected_topic="calculus",
        typing_speed_ratio=1.0,
        deletion_rate=0.0,
        pause_duration=0.0,
        scroll_back_count=0,
        user_touched_agent=True,
        user_message="show me what this integral looks like graphically",
        user_id="test",
    )
    result_c = detector.score(ctx_c)
    print(f"  Scenario C (explicit touch + visual keywords):")
    print(f"    Score:          {result_c.confusion_score:.2f}")
    print(f"    Confusion type: {result_c.confusion_type}")
    assert result_c.confusion_type == "VISUAL_SPATIAL", f"Expected VISUAL_SPATIAL, got {result_c.confusion_type}"
    print("  ‚úì PASS ‚Äî routed to VISUAL_SPATIAL")


async def test_context_post():
    """Test 3: POST context to backend (simulates Chrome extension)."""
    print("\n" + "=" * 50)
    print("TEST 3: POST Context (simulates Chrome extension)")
    print("=" * 50)

    context_payload = {
        "screen_content": "def train_model(X, y, lr=0.01):\n    weights = np.zeros(X.shape[1])\n    for i in range(1000):\n        pred = X @ weights\n        grad = X.T @ (pred - y) / len(y)\n        weights -= lr * grad",
        "screen_content_type": "code",
        "detected_topic": "gradient_descent",
        "detected_subtopic": "learning_rate",
        "typing_speed_ratio": 0.25,    # very slow
        "deletion_rate": 0.6,          # lots of deletions
        "pause_duration": 20.0,        # long pause
        "scroll_back_count": 8,        # re-reading a lot
        "user_touched_agent": False,
        "user_message": "",
        "user_id": "test_user_1",
        "session_id": f"test_session_{int(time.time())}",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
    }

    async with httpx.AsyncClient() as client:
        resp = await client.post(
            f"{BACKEND}/context",
            json=context_payload,
            timeout=5.0,
        )
        print(f"  POST /context status: {resp.status_code}")
        print(f"  Response: {resp.json()}")

        # Check latest context
        resp2 = await client.get(f"{BACKEND}/context/latest", timeout=5.0)
        data = resp2.json()
        print(f"  GET /context/latest: topic={data.get('detected_topic')}, "
              f"typing={data.get('typing_speed_ratio')}")

    assert resp.status_code == 200
    print("  ‚úì PASS ‚Äî context accepted by backend")


async def test_context_with_screenshot():
    """Test 4: POST context WITH screenshot (simulates vision input)."""
    print("\n" + "=" * 50)
    print("TEST 4: Context with Screenshot (vision pipeline)")
    print("=" * 50)

    has_key = bool(os.environ.get("GEMINI_API_KEY"))
    print(f"  GEMINI_API_KEY set: {has_key}")

    screenshot = make_fake_screenshot()

    context_payload = {
        "screen_content": "loss = -sum(y * log(p) + (1-y) * log(1-p))",
        "screen_content_type": "equation",
        "detected_topic": "classification",
        "screenshot_b64": screenshot,
        "typing_speed_ratio": 0.4,
        "deletion_rate": 0.3,
        "pause_duration": 12.0,
        "scroll_back_count": 3,
        "user_touched_agent": False,
        "user_message": "",
        "user_id": "test_user_1",
        "session_id": f"test_vision_{int(time.time())}",
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
    }

    async with httpx.AsyncClient() as client:
        resp = await client.post(
            f"{BACKEND}/context",
            json=context_payload,
            timeout=5.0,
        )
        print(f"  POST /context (with screenshot) status: {resp.status_code}")

    if has_key:
        print("  ‚Üí Gemini Vision API will analyze the screenshot")
        print("  ‚Üí Check the run.py terminal for screen analysis logs")
    else:
        print("  ‚Üí Screenshot posted but Gemini Vision won't run (no API key)")
        print("  ‚Üí Confusion detection still works from behavioral signals alone")

    print("  ‚úì PASS ‚Äî context with screenshot accepted")


async def test_explicit_touch():
    """Test 5: Explicit help request (user touches the agent dot)."""
    print("\n" + "=" * 50)
    print("TEST 5: Explicit Help Request (touch)")
    print("=" * 50)

    async with httpx.AsyncClient() as client:
        resp = await client.post(
            f"{BACKEND}/touch",
            json={
                "message": "I don't understand why the loss function uses log",
                "user_id": "test_user_1",
            },
            timeout=5.0,
        )
        print(f"  POST /touch status: {resp.status_code}")
        print(f"  Response: {resp.json()}")

    assert resp.status_code == 200
    print("  ‚úì PASS ‚Äî touch request accepted")


async def test_bkt_tracking():
    """Test 6: BKT model tracks concept mastery."""
    print("\n" + "=" * 50)
    print("TEST 6: BKT Mastery Tracking")
    print("=" * 50)

    from agents.learner_model import ConfidenceWeightedBKT

    bkt = ConfidenceWeightedBKT()

    # Simulate learning gradient_descent
    bkt.init_concept("gradient_descent")
    print(f"  Initial mastery: {bkt.get_mastery('gradient_descent'):.3f}")

    # Correct observations with increasing confidence
    for i in range(8):
        confidence = min(0.5 + i * 0.1, 0.95)
        bkt.update("gradient_descent", correct=True, confidence=confidence)
        m = bkt.get_mastery("gradient_descent")
        print(f"  After correct #{i+1} (conf={confidence:.2f}): mastery={m:.3f}")

    final = bkt.get_mastery("gradient_descent")
    print(f"  Final mastery: {final:.3f}")
    print(f"  Is mastered: {bkt.is_mastered('gradient_descent')}")
    assert final > 0.7, "Expected mastery to increase with correct observations"
    print("  ‚úì PASS ‚Äî BKT tracks learning progress")


async def test_agent_routing_logic():
    """Test 7: Full routing decision (confusion ‚Üí correct agent type)."""
    print("\n" + "=" * 50)
    print("TEST 7: Agent Routing Logic")
    print("=" * 50)

    from agents.confusion_detector import ConfusionDetector
    from agents.models import WorkContext

    detector = ConfusionDetector()

    scenarios = [
        {
            "name": "PROCEDURAL_HOW (high deletion)",
            "ctx": WorkContext(
                screen_content="for i in range(len(data)):",
                screen_content_type="code",
                typing_speed_ratio=0.5,
                deletion_rate=0.8,
                pause_duration=3.0,
                scroll_back_count=0,
                user_touched_agent=False,
                user_id="test",
            ),
            "expected": "PROCEDURAL_HOW",
        },
        {
            "name": "CONCEPTUAL_WHY (pause + verbal)",
            "ctx": WorkContext(
                screen_content="The chain rule states that d/dx[f(g(x))] = f'(g(x)) * g'(x)",
                screen_content_type="text",
                typing_speed_ratio=0.4,
                deletion_rate=0.1,
                pause_duration=20.0,
                scroll_back_count=3,
                user_touched_agent=False,
                user_message="wait what",
                user_id="test",
            ),
            "expected": "CONCEPTUAL_WHY",
        },
        {
            "name": "VISUAL_SPATIAL (equation + re-reading)",
            "ctx": WorkContext(
                screen_content="‚àáf(x) = [‚àÇf/‚àÇx‚ÇÅ, ‚àÇf/‚àÇx‚ÇÇ, ..., ‚àÇf/‚àÇx‚Çô]",
                screen_content_type="equation",
                typing_speed_ratio=1.0,
                deletion_rate=0.0,
                pause_duration=5.0,
                scroll_back_count=10,
                user_touched_agent=False,
                user_id="test",
            ),
            "expected": "VISUAL_SPATIAL",
        },
        {
            "name": "EXPLICIT ‚Üí VISUAL_SPATIAL (touch + visual keywords)",
            "ctx": WorkContext(
                screen_content="matrix multiplication",
                screen_content_type="text",
                typing_speed_ratio=1.0,
                user_touched_agent=True,
                user_message="show me a graph of this",
                user_id="test",
            ),
            "expected": "VISUAL_SPATIAL",
        },
        {
            "name": "EXPLICIT ‚Üí CONCEPTUAL_WHY (touch + conceptual keyword)",
            "ctx": WorkContext(
                screen_content="backpropagation algorithm",
                screen_content_type="text",
                typing_speed_ratio=1.0,
                user_touched_agent=True,
                user_message="why does this work?",
                user_id="test",
            ),
            "expected": "CONCEPTUAL_WHY",
        },
    ]

    all_pass = True
    for s in scenarios:
        result = detector.score(s["ctx"])
        status = "‚úì" if result.confusion_type == s["expected"] else "‚úó"
        if status == "‚úó":
            all_pass = False
        print(f"  {status} {s['name']}: got {result.confusion_type} "
              f"(score={result.confusion_score:.2f})")

    assert all_pass, "Some routing decisions were wrong"
    print("  ‚úì ALL ROUTING TESTS PASS")


async def main():
    print("=" * 50)
    print("  AMBIENT LEARNING ‚Äî PIPELINE TEST")
    print("=" * 50)
    print(f"  GEMINI_API_KEY: {'SET ‚úì' if os.environ.get('GEMINI_API_KEY') else 'NOT SET (partial testing)'}")
    print(f"  Backend: {BACKEND}")

    # Tests that DON'T need the backend running
    await test_confusion_detection_local()
    await test_bkt_tracking()
    await test_agent_routing_logic()

    # Tests that need `python run.py` running
    print("\n" + "=" * 50)
    print("  BACKEND INTEGRATION TESTS")
    print("  (require `python run.py` in another terminal)")
    print("=" * 50)

    try:
        await test_health()
        await test_context_post()
        await test_context_with_screenshot()
        await test_explicit_touch()
    except httpx.ConnectError:
        print("\n  ‚ö† Backend not running! Start it with: python run.py")
        print("  Skipping integration tests...")

    # Summary
    print("\n" + "=" * 50)
    print("  SUMMARY")
    print("=" * 50)
    has_key = bool(os.environ.get("GEMINI_API_KEY"))
    print("  Local tests (no backend needed):")
    print("    ‚úì Confusion detection works")
    print("    ‚úì BKT mastery tracking works")
    print("    ‚úì Agent routing logic works")
    print()
    if has_key:
        print("  With GEMINI_API_KEY:")
        print("    ‚úì Gemini Vision can analyze screenshots")
        print("    ‚úì Deep Diver can generate Socratic dialogue")
        print("    ‚úì Assessor can generate contrastive challenges")
        print("    ‚úì Visualizer can generate scene descriptions")
    else:
        print("  Without GEMINI_API_KEY:")
        print("    ‚úì Confusion detection ‚Üí agent routing works")
        print("    ‚úì Behavioral signals are scored correctly")
        print("    ‚úó Gemini Vision (screenshot analysis) ‚Äî needs key")
        print("    ‚úó Agent responses (deep_diver, assessor, visualizer) ‚Äî needs key")
        print()
        print("  ‚Üí Next step: export GEMINI_API_KEY=your_key_here")
        print("    Then re-run this test to verify Gemini integration")


if __name__ == "__main__":
    asyncio.run(main())
